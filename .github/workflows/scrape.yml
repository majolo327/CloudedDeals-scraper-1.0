name: Daily Scraper

# Dynamic run name — each scheduled region is identifiable at a glance in the
# Actions UI (e.g. "Scrape — nevada", "Scrape — michigan").
# Uses contains() instead of == for resilient cron-expression matching.
run-name: >-
  Scrape — ${{
    github.event_name == 'workflow_dispatch'
      && format('{0}{1}', github.event.inputs.region || 'all',
           github.event.inputs.dry_run == 'true' && ' (dry-run)' || '')
    || contains(github.event.schedule || '', '30 11') && 'massachusetts'
    || contains(github.event.schedule || '', '0 11') && 'pennsylvania'
    || contains(github.event.schedule || '', '30 12') && 'new-jersey'
    || contains(github.event.schedule || '', '0 12') && 'new-york'
    || contains(github.event.schedule || '', '30 13') && 'michigan (1/4)'
    || contains(github.event.schedule || '', '32 13') && 'michigan (2/4)'
    || contains(github.event.schedule || '', '34 13') && 'michigan (3/4)'
    || contains(github.event.schedule || '', '36 13') && 'michigan (4/4)'
    || contains(github.event.schedule || '', '0 13') && 'ohio'
    || contains(github.event.schedule || '', '30 14') && 'arizona'
    || contains(github.event.schedule || '', '30 15') && 'colorado'
    || contains(github.event.schedule || '', '0 15') && 'illinois'
    || contains(github.event.schedule || '', '5 16') && 'southern-nv'
    || contains(github.event.schedule || '', '0 17') && 'missouri'
    || 'scheduled'
  }}

on:
  schedule:
    # Optimized cron schedule — all 11 states.
    # East Coast expansion runs first (6-8 AM EST), then Midwest/West,
    # then NV anchor at 8:05 AM PST so Vegas has fresh data at open.
    # GHA crons typically run 15-30 min late — account for that.
    #
    # ── East Coast (6-8 AM EST) ─────────────────────────────────────
    - cron: "0 11 * * *"    # pennsylvania   — 6:00 AM EST  (16 sites, ~15 min)
    - cron: "30 11 * * *"   # massachusetts  — 6:30 AM EST  (17 sites, ~15 min)
    - cron: "0 12 * * *"    # new-york       — 7:00 AM EST  (18 sites, ~15 min)
    - cron: "30 12 * * *"   # new-jersey     — 7:30 AM EST  (34 sites, ~25 min)
    - cron: "0 13 * * *"    # ohio           — 8:00 AM EST  (22 sites, ~15 min)
    #
    # ── Midwest / Great Lakes ───────────────────────────────────────
    # Michigan is sharded into 4 parallel jobs (~50 sites each).
    - cron: "30 13 * * *"   # michigan-1     — 8:30 AM EST  (shard 1/4, ~50 sites)
    - cron: "32 13 * * *"   # michigan-2     — 8:32 AM EST  (shard 2/4, ~50 sites)
    - cron: "34 13 * * *"   # michigan-3     — 8:34 AM EST  (shard 3/4, ~50 sites)
    - cron: "36 13 * * *"   # michigan-4     — 8:36 AM EST  (shard 4/4, ~50 sites)
    - cron: "0 15 * * *"    # illinois       — 9:00 AM CST  (88 sites, ~40 min)
    #
    # ── Mountain / West ─────────────────────────────────────────────
    - cron: "30 14 * * *"   # arizona        — 7:30 AM MST  (52 sites, ~30 min)
    - cron: "30 15 * * *"   # colorado       — 8:30 AM MST  (17 sites, ~15 min)
    #
    # ── NV anchor — fresh data by Vegas open ────────────────────────
    - cron: "5 16 * * *"    # southern-nv    — 8:05 AM PST  (63 sites, ~35 min) ★
    #
    # ── Tail ────────────────────────────────────────────────────────
    - cron: "0 17 * * *"    # missouri       — 11:00 AM CST (31 sites, ~20 min)
  workflow_dispatch:
    inputs:
      platform_group:
        description: "Platform group to scrape"
        required: false
        type: choice
        options:
          - all
          - stable
          - new
        default: "all"
      region:
        description: "Region/state to scrape (michigan shards: michigan-1..4 ~50 sites each)"
        required: false
        type: choice
        options:
          - all
          - southern-nv
          - michigan
          - michigan-1
          - michigan-2
          - michigan-3
          - michigan-4
          - illinois
          - arizona
          - missouri
          - new-jersey
          - ohio
          - colorado
          - new-york
          - massachusetts
          - pennsylvania
        default: "all"
      dry_run:
        description: "Dry run (scrape but don't write to DB)"
        required: false
        type: boolean
        default: false
      limit_dispensaries:
        description: "Test with limited dispensaries (1 per platform)"
        required: false
        type: boolean
        default: false
      single_site:
        description: "Scrape a single site by slug (e.g. td-gibson)"
        required: false
        type: string
        default: ""

# Concurrency is scoped per-region so each state runs independently.
# Scheduled runs key off the cron expression; manual runs key off region input.
concurrency:
  group: scraper-${{ github.event.schedule || github.event.inputs.region || 'default' }}
  cancel-in-progress: false

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 120

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Determine region
        id: region
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "value=${{ github.event.inputs.region || 'all' }}" >> $GITHUB_OUTPUT
          elif [ "${{ github.event.schedule }}" = "0 11 * * *" ]; then
            echo "value=pennsylvania" >> $GITHUB_OUTPUT
          elif [ "${{ github.event.schedule }}" = "30 11 * * *" ]; then
            echo "value=massachusetts" >> $GITHUB_OUTPUT
          elif [ "${{ github.event.schedule }}" = "0 12 * * *" ]; then
            echo "value=new-york" >> $GITHUB_OUTPUT
          elif [ "${{ github.event.schedule }}" = "30 12 * * *" ]; then
            echo "value=new-jersey" >> $GITHUB_OUTPUT
          elif [ "${{ github.event.schedule }}" = "0 13 * * *" ]; then
            echo "value=ohio" >> $GITHUB_OUTPUT
          elif [ "${{ github.event.schedule }}" = "30 13 * * *" ]; then
            echo "value=michigan-1" >> $GITHUB_OUTPUT
          elif [ "${{ github.event.schedule }}" = "32 13 * * *" ]; then
            echo "value=michigan-2" >> $GITHUB_OUTPUT
          elif [ "${{ github.event.schedule }}" = "34 13 * * *" ]; then
            echo "value=michigan-3" >> $GITHUB_OUTPUT
          elif [ "${{ github.event.schedule }}" = "36 13 * * *" ]; then
            echo "value=michigan-4" >> $GITHUB_OUTPUT
          elif [ "${{ github.event.schedule }}" = "30 14 * * *" ]; then
            echo "value=arizona" >> $GITHUB_OUTPUT
          elif [ "${{ github.event.schedule }}" = "0 15 * * *" ]; then
            echo "value=illinois" >> $GITHUB_OUTPUT
          elif [ "${{ github.event.schedule }}" = "30 15 * * *" ]; then
            echo "value=colorado" >> $GITHUB_OUTPUT
          elif [ "${{ github.event.schedule }}" = "0 17 * * *" ]; then
            echo "value=missouri" >> $GITHUB_OUTPUT
          else
            echo "value=southern-nv" >> $GITHUB_OUTPUT
          fi

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"
          cache-dependency-path: clouded-deals/scraper/requirements.txt

      - name: Cache Playwright browsers
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-playwright-${{ hashFiles('clouded-deals/scraper/requirements.txt') }}

      - name: Install dependencies
        working-directory: clouded-deals/scraper
        run: |
          pip install -r requirements.txt
          playwright install-deps
          playwright install chromium

      - name: Test Supabase connection
        working-directory: clouded-deals/scraper
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
        run: python test_connection.py

      - name: Run scraper
        working-directory: clouded-deals/scraper
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          DRY_RUN: ${{ github.event.inputs.dry_run || 'false' }}
          LIMIT_DISPENSARIES: ${{ github.event.inputs.limit_dispensaries || 'false' }}
          SINGLE_SITE: ${{ github.event.inputs.single_site || '' }}
          # Force run for manual dispatch and non-NV scheduled regions
          # (idempotency check doesn't track per-region yet).
          FORCE_RUN: ${{ github.event_name == 'workflow_dispatch' && 'true' || steps.region.outputs.value != 'southern-nv' && 'true' || 'false' }}
          PLATFORM_GROUP: ${{ github.event.inputs.platform_group || 'stable' }}
          REGION: ${{ steps.region.outputs.value }}
        run: |
          echo "=== Scraper Configuration ==="
          echo "Group:      $PLATFORM_GROUP"
          echo "Region:     $REGION"
          echo "Dry run:    $DRY_RUN"
          echo "Limited:    $LIMIT_DISPENSARIES"
          echo "Single:     $SINGLE_SITE"
          echo "Force:      $FORCE_RUN"
          echo "============================="

          if [ -n "$SINGLE_SITE" ]; then
            python main.py "$SINGLE_SITE"
          else
            python main.py
          fi

      - name: Show scrape summary
        if: always()
        working-directory: clouded-deals/scraper
        run: |
          if [ -f scrape_summary.txt ]; then
            echo "### Scrape Summary" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat scrape_summary.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo ""
            echo "=== Scrape Summary ==="
            cat scrape_summary.txt
          else
            echo "No scrape_summary.txt found (scraper may have crashed before report)"
          fi

      - name: Upload debug screenshots
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug-screenshots-${{ steps.region.outputs.value }}-${{ github.run_id }}
          path: |
            clouded-deals/scraper/debug_screenshots/
          retention-days: 7
          if-no-files-found: ignore

      - name: Upload scrape summary
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scrape-summary-${{ github.event.inputs.platform_group || 'stable' }}-${{ github.run_id }}
          path: |
            clouded-deals/scraper/scrape_summary.txt
          retention-days: 30
          if-no-files-found: ignore

      - name: Upload logs on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-${{ steps.region.outputs.value }}-${{ github.run_id }}
          path: |
            clouded-deals/scraper/*.log
          retention-days: 7
