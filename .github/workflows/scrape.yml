name: Daily Scraper

on:
  schedule:
    # Cron targets 7:00 AM PST / 8:00 AM PDT (15:00 UTC). GitHub Actions
    # cron typically runs 15-30 min late, so actual start lands ~7:15-7:30
    # AM PST (winter) or ~8:15-8:30 AM PDT (summer).
    - cron: "0 15 * * *"
  workflow_dispatch:
    inputs:
      platform_group:
        description: "Platform group to scrape"
        required: false
        type: choice
        options:
          - all
          - stable
          - new
        default: "all"
      dry_run:
        description: "Dry run (scrape but don't write to DB)"
        required: false
        type: boolean
        default: false
      limit_dispensaries:
        description: "Test with limited dispensaries (1 per platform)"
        required: false
        type: boolean
        default: false
      single_site:
        description: "Scrape a single site by slug (e.g. td-gibson)"
        required: false
        type: string
        default: ""

# Concurrency is scoped by platform group so "stable" and "new" can
# run in parallel without cancelling each other.
concurrency:
  group: scraper-${{ github.event.inputs.platform_group || 'stable' }}
  cancel-in-progress: false

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 120

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"
          cache-dependency-path: clouded-deals/scraper/requirements.txt

      - name: Cache Playwright browsers
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-playwright-${{ hashFiles('clouded-deals/scraper/requirements.txt') }}

      - name: Install dependencies
        working-directory: clouded-deals/scraper
        run: |
          pip install -r requirements.txt
          playwright install-deps
          playwright install chromium

      - name: Test Supabase connection
        working-directory: clouded-deals/scraper
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
        run: python test_connection.py

      - name: Run scraper
        working-directory: clouded-deals/scraper
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          DRY_RUN: ${{ github.event.inputs.dry_run || 'false' }}
          LIMIT_DISPENSARIES: ${{ github.event.inputs.limit_dispensaries || 'false' }}
          SINGLE_SITE: ${{ github.event.inputs.single_site || '' }}
          FORCE_RUN: ${{ github.event_name == 'workflow_dispatch' && 'true' || 'false' }}
          # Cron runs "stable" only; manual dispatch uses the selected group.
          PLATFORM_GROUP: ${{ github.event.inputs.platform_group || 'stable' }}
        run: |
          echo "=== Scraper Configuration ==="
          echo "Group:      $PLATFORM_GROUP"
          echo "Dry run:    $DRY_RUN"
          echo "Limited:    $LIMIT_DISPENSARIES"
          echo "Single:     $SINGLE_SITE"
          echo "Force:      $FORCE_RUN"
          echo "============================="

          if [ -n "$SINGLE_SITE" ]; then
            python main.py "$SINGLE_SITE"
          else
            python main.py
          fi

      - name: Show scrape summary
        if: always()
        working-directory: clouded-deals/scraper
        run: |
          if [ -f scrape_summary.txt ]; then
            echo "### Scrape Summary" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat scrape_summary.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo ""
            echo "=== Scrape Summary ==="
            cat scrape_summary.txt
          else
            echo "No scrape_summary.txt found (scraper may have crashed before report)"
          fi

      - name: Upload debug screenshots
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug-screenshots-${{ github.event.inputs.platform_group || 'stable' }}-${{ github.run_id }}
          path: |
            clouded-deals/scraper/debug_screenshots/
          retention-days: 7
          if-no-files-found: ignore

      - name: Upload scrape summary
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scrape-summary-${{ github.event.inputs.platform_group || 'stable' }}-${{ github.run_id }}
          path: |
            clouded-deals/scraper/scrape_summary.txt
          retention-days: 30
          if-no-files-found: ignore

      - name: Upload logs on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-${{ github.event.inputs.platform_group || 'stable' }}-${{ github.run_id }}
          path: |
            clouded-deals/scraper/*.log
          retention-days: 7
